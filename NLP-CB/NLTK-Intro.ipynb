{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=brown.sents(categories='editorial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2997"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Assembly', 'session', 'brought', 'much', 'good'], ['The', 'General', 'Assembly', ',', 'which', 'adjourns', 'today', ',', 'has', 'performed', 'in', 'an', 'atmosphere', 'of', 'crisis', 'and', 'struggle', 'from', 'the', 'day', 'it', 'convened', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Where can I find past ACM ICPC regionals and finals questions with solutions?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sent_tokenize(sent.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(word_tokenize(sents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'finals', 'with', 'and', 'regionals', 'questions', 'find', 'where', 'acm', 'icpc', 'can', '?', 'past', 'solutions', 'i'}\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['finals', 'regionals', 'questions', 'find', 'acm', 'icpc', '?', 'past', 'solutions']\n"
     ]
    }
   ],
   "source": [
    "useful_words = [w for w in words if w not in sw]\n",
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[a-zA-Z@]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', 'there', 'how', 'are', 'you']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hey there! how are you?\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "- Process that transforms words(verbs, plurals) into their radical form \n",
    "- Preserve the semantics of the sentence without increasing the number of unique tokens\n",
    "- Jumps, jumping, jumped, jump ==> jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foxes', 'love', 'to', 'make', 'jumps', 'the', 'quick', 'brown', 'fox', 'was', 'seen', 'jumping', 'over', 'the', 'lovely', 'dog', 'from', 'a', 'ft', 'high', 'wall']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Foxes love to make jumps. The quick brown fox was seen jumping over\n",
    "        the lovely dog from a 6ft high wall.\"\"\"\n",
    "word_list = tokenizer.tokenize(text.lower())\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(words, sw):\n",
    "    useful_words = [w for w in words if w not in sw]\n",
    "    return useful_words\n",
    "\n",
    "word_list = filter_words(word_list, sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foxes',\n",
       " 'love',\n",
       " 'make',\n",
       " 'jumps',\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'seen',\n",
       " 'jumping',\n",
       " 'lovely',\n",
       " 'dog',\n",
       " 'ft',\n",
       " 'high',\n",
       " 'wall']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "- Snowball Stemmer\n",
    "- Porter Stemmer\n",
    "- Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"Jumps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"Lovely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lov'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.stem(\"Lovely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem(\"Lovely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cry'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(\"crying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a common vocabulary and vectorizing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Presenting the second video song \"Dekhte Dekhte\" from the upcoming Hindi movie \"Batti Gul Meter Chalu\".',\n",
    "    'This Bollywood rendition of Nusrat Fateh Ali Khan original song is done by Rochak Kohli and sung by Atif Aslam.',\n",
    "    'Presenting the first official song Milegi Milegi sung by Mika Singh from the movie Stree.',\n",
    "    'Stree is a first of its kind horror comedy, inspired from a true phenomenon'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Presenting the second video song \"Dekhte Dekhte\" from the upcoming Hindi movie \"Batti Gul Meter Chalu\".', 'This Bollywood rendition of Nusrat Fateh Ali Khan original song is done by Rochak Kohli and sung by Atif Aslam.', 'Presenting the first official song Milegi Milegi sung by Mika Singh from the movie Stree.', 'Stree is a first of its kind horror comedy, inspired from a true phenomenon']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2, 0,\n",
       "         0, 1, 1],\n",
       "        [1, 1, 1, 1, 0, 1, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "         0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 2, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'presenting': 32, 'the': 40, 'second': 35, 'video': 44, 'song': 37, 'dekhte': 9, 'from': 13, 'upcoming': 43, 'hindi': 15, 'movie': 26, 'batti': 4, 'gul': 14, 'meter': 23, 'chalu': 7, 'this': 41, 'bollywood': 5, 'rendition': 33, 'of': 28, 'nusrat': 27, 'fateh': 11, 'ali': 0, 'khan': 20, 'original': 30, 'is': 18, 'done': 10, 'by': 6, 'rochak': 34, 'kohli': 22, 'and': 1, 'sung': 39, 'atif': 3, 'aslam': 2, 'first': 12, 'official': 29, 'milegi': 25, 'mika': 24, 'singh': 36, 'stree': 38, 'its': 19, 'kind': 21, 'horror': 16, 'comedy': 8, 'inspired': 17, 'true': 42, 'phenomenon': 31}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Given a vector what is the sentence\n",
    "import numpy as np\n",
    "vector = np.ones((37,))\n",
    "vector[3:7] = 0\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['ali', 'and', 'aslam', 'chalu', 'comedy', 'dekhte', 'done',\n",
      "       'fateh', 'first', 'from', 'gul', 'hindi', 'horror', 'inspired',\n",
      "       'is', 'its', 'khan', 'kind', 'kohli', 'meter', 'mika', 'milegi',\n",
      "       'movie', 'nusrat', 'of', 'official', 'original', 'phenomenon',\n",
      "       'presenting', 'rendition', 'rochak', 'second', 'singh'],\n",
      "      dtype='<U10')]\n"
     ]
    }
   ],
   "source": [
    "print(cv.inverse_transform(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Effectively reduce the size of the vector\n",
    "def myTokenizer(sentence):\n",
    "    sw = set(stopwords.words('english'))\n",
    "    words = tokenizer.tokenize(sentence.lower())\n",
    "    return filter_words(words, sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['presenting',\n",
       " 'second',\n",
       " 'video',\n",
       " 'song',\n",
       " 'dekhte',\n",
       " 'dekhte',\n",
       " 'upcoming',\n",
       " 'hindi',\n",
       " 'movie',\n",
       " 'batti',\n",
       " 'gul',\n",
       " 'meter',\n",
       " 'chalu']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTokenizer(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 1 0 2 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1]\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer)\n",
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "vc = vectorized_corpus.toarray()\n",
    "print(vc[0])\n",
    "print(len(vc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['batti', 'chalu', 'dekhte', 'gul', 'hindi', 'meter', 'movie',\n",
       "        'presenting', 'second', 'song', 'upcoming', 'video'], dtype='<U10')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = vc[0]\n",
    "cv.inverse_transform(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features in Bag of words model\n",
    "- Unigram\n",
    "- Bigram\n",
    "- Trigram\n",
    "- N gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'presenting': 77, 'second': 88, 'video': 114, 'song': 94, 'dekhte': 16, 'upcoming': 111, 'hindi': 35, 'movie': 63, 'batti': 6, 'gul': 32, 'meter': 53, 'chalu': 12, 'presenting second': 80, 'second video': 89, 'video song': 115, 'song dekhte': 95, 'dekhte dekhte': 17, 'dekhte upcoming': 19, 'upcoming hindi': 112, 'hindi movie': 36, 'movie batti': 64, 'batti gul': 7, 'gul meter': 33, 'meter chalu': 54, 'presenting second video': 81, 'second video song': 90, 'video song dekhte': 116, 'song dekhte dekhte': 96, 'dekhte dekhte upcoming': 18, 'dekhte upcoming hindi': 20, 'upcoming hindi movie': 113, 'hindi movie batti': 37, 'movie batti gul': 65, 'batti gul meter': 8, 'gul meter chalu': 34, 'bollywood': 9, 'rendition': 82, 'nusrat': 67, 'fateh': 24, 'ali': 0, 'khan': 44, 'original': 73, 'done': 21, 'rochak': 85, 'kohli': 50, 'sung': 104, 'atif': 4, 'aslam': 3, 'bollywood rendition': 10, 'rendition nusrat': 83, 'nusrat fateh': 68, 'fateh ali': 25, 'ali khan': 1, 'khan original': 45, 'original song': 74, 'song done': 97, 'done rochak': 22, 'rochak kohli': 86, 'kohli sung': 51, 'sung atif': 105, 'atif aslam': 5, 'bollywood rendition nusrat': 11, 'rendition nusrat fateh': 84, 'nusrat fateh ali': 69, 'fateh ali khan': 26, 'ali khan original': 2, 'khan original song': 46, 'original song done': 75, 'song done rochak': 98, 'done rochak kohli': 23, 'rochak kohli sung': 87, 'kohli sung atif': 52, 'sung atif aslam': 106, 'first': 27, 'official': 70, 'milegi': 58, 'mika': 55, 'singh': 91, 'stree': 101, 'presenting first': 78, 'first official': 30, 'official song': 71, 'song milegi': 99, 'milegi milegi': 59, 'milegi sung': 61, 'sung mika': 107, 'mika singh': 56, 'singh movie': 92, 'movie stree': 66, 'presenting first official': 79, 'first official song': 31, 'official song milegi': 72, 'song milegi milegi': 100, 'milegi milegi sung': 60, 'milegi sung mika': 62, 'sung mika singh': 108, 'mika singh movie': 57, 'singh movie stree': 93, 'kind': 47, 'horror': 38, 'comedy': 13, 'inspired': 41, 'true': 109, 'phenomenon': 76, 'stree first': 102, 'first kind': 28, 'kind horror': 48, 'horror comedy': 39, 'comedy inspired': 14, 'inspired true': 42, 'true phenomenon': 110, 'stree first kind': 103, 'first kind horror': 29, 'kind horror comedy': 49, 'horror comedy inspired': 40, 'comedy inspired true': 15, 'inspired true phenomenon': 43}\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer, ngram_range=(1,3))\n",
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "vc = vectorized_corpus.toarray()\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 2 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      "  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0\n",
      "  0 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 1 0\n",
      "  0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 2 1 1 1 1 1 0 0 1 0 0 0 1 1\n",
      "  1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 0 1\n",
      "  1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
      "  0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0\n",
      "  0 1 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vc) # frequency of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF Normalization\n",
    "- Avoid features that occur very often, because they contain less information\n",
    "- Information decreases as the number of occurences increases across different type of document\n",
    "- So we define the term , term-document-frequency which associates a term with its frequeny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdidf_vectorizer = TfidfVectorizer(tokenizer=myTokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
